{"cells":[{"cell_type":"markdown","metadata":{"id":"2iYLQO5Evvqy"},"source":["# 1️⃣ Training an Adapter for a Transformer model\n","\n","In this notebook, we train an adapter for a **RoBERTa** ([Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf)) model for sequence classification on a **sentiment analysis** task using [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers), the _AdapterHub_ adaptation of HuggingFace's _transformers_ library.\n","\n","If you're unfamiliar with the theoretical parts of adapters or the AdapterHub framework, check out our [introductory blog post](https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/) first.\n","\n","We train a **Task Adapter** for a pre-trained model here. Most of the code is identical to a full finetuning setup using HuggingFace's transformers. For comparison, have a look at the [same guide using full finetuning](https://colab.research.google.com/drive/1brXJg5Mokm8h3shxqPRnoIsRwHQoncus?usp=sharing).\n","\n","For training, we use the [movie review dataset by Pang and Lee (2005)](http://www.cs.cornell.edu/people/pabo/movie-review-data/). It contains movie reviews  from Rotten Tomatoes which are either classified as positive or negative. We download the dataset via HuggingFace's [datasets](https://github.com/huggingface/datasets) library."]},{"cell_type":"markdown","metadata":{"id":"a-XTIOLv0isn"},"source":["## Installation\n","\n","First, let's install the required libraries:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFI6QsqzfZ5H","executionInfo":{"status":"ok","timestamp":1639536922525,"user_tz":300,"elapsed":1386,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}},"outputId":"89e7db20-1cdf-493e-97d5-80d31c014258"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ju-alwbHmKYA","outputId":"be3a100a-a642-4520-e152-4d5c657dd7f0","pycharm":{"is_executing":false},"executionInfo":{"status":"ok","timestamp":1639536929342,"user_tz":300,"elapsed":6824,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: adapter-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.8.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.0.46)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->adapter-transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->adapter-transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install -U adapter-transformers\n","!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"7Mx916lBCfoL"},"source":["## Dataset Preprocessing\n","\n","Before we start to train our adapter, we first prepare the training data. Our training dataset can be loaded via HuggingFace `datasets` using one line of code:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["a9e0c4cc805347be8ea8724d84b3153d","d0b3f2e343c04d31934f9af5eff832ed","02e58f4e62fe4d5fa7bb4e3b510549fc","39fb423a6e9246df9536c8963e23d564","dabc9f8c003d4066ab03b721d208cf52","9650b69982ca41f9915d9546ccaa161b","2ea18605f8b948c3ad7606d74e8d7260","36cdb0b7705e4b7aa6d282fc1f730b18","dca63249a5724b0a840b2c542707622b","edd5887dd82f484199b783256d21a936","dbcac12ef3ab4df88a9309a0f5ff8da8"]},"id":"DYHRQKkM5xhl","outputId":"917a52be-5567-423d-d57d-146dc1e18d2b","pycharm":{"is_executing":false},"executionInfo":{"status":"ok","timestamp":1639536932263,"user_tz":300,"elapsed":567,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default\n","Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9e0c4cc805347be8ea8724d84b3153d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'test': 7600, 'train': 120000}"]},"metadata":{},"execution_count":9}],"source":["from datasets import load_dataset, DatasetDict\n","\n","dataset = load_dataset(\"ag_news\")\n","dataset.num_rows"]},{"cell_type":"markdown","metadata":{"id":"UC-rOkx-DbVD"},"source":["Every dataset sample has an input text and a binary label:"]},{"cell_type":"code","source":["max([len(x['text']) for x in dataset['train']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwT8tbCq1UPr","outputId":"f000777d-52cc-4f6c-bc42-0ce2c0d09bd5","executionInfo":{"status":"ok","timestamp":1639536942311,"user_tz":300,"elapsed":6677,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1012"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbF_rq_lAqcS","outputId":"f83f52c0-a36c-4f57-9363-c2895797559f","executionInfo":{"status":"ok","timestamp":1639536944937,"user_tz":300,"elapsed":143,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 120000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 7600\n","    })\n","})"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"gOeIvXAyEIOa"},"source":["Now, we need to encode all dataset samples to valid inputs for our Transformer model. Since we want to train on `roberta-base`, we load the corresponding `RobertaTokenizer`. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["8302c11f705d406392ec6d0911944dc6","c486926c5c0c46c485a086d213f819bc","ff93d2b908f34b9ab8fb11aa72505883","704f16ac62d94f459307a94ddec0428b","9c813a83d84044bfa4d7aadf2a00546e","2d4f59f1d3514539b4f5bb6ec3a91a4d","bdec0bd6544340e9aa331608a5f9d2a1","61db1d37547d4c34bd0bc7314697b906","013ac438b1a541258bec00fc074073af","eea73dd7cb16451c955b46bbdd892db5","ce0e3c61e8654d14aa727e8a6abfc988"]},"id":"_xdVDIc58O6g","outputId":"5a0e3c7a-0cd8-4399-ed61-a3db1e3d5381","pycharm":{"is_executing":false},"executionInfo":{"status":"ok","timestamp":1639536953869,"user_tz":300,"elapsed":7432,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-1c11d29ee2e50e78.arrow\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8302c11f705d406392ec6d0911944dc6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"]},"metadata":{}}],"source":["from transformers import RobertaTokenizer\n","\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","def encode_batch(batch):\n","  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n","  return tokenizer(batch[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n","\n","# Encode the input data\n","dataset = dataset.map(encode_batch, batched=True)\n","# The transformers model expects the target class column to be named \"labels\"\n","dataset.rename_column_(\"label\", \"labels\")\n","# Transform to pytorch tensors and only output the required columns\n","dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"]},{"cell_type":"markdown","metadata":{"id":"O0Nm9ke6GgR-"},"source":["Now we're ready to train our model..."]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8yIUqhg3Ixe","outputId":"95a69510-fd97-42a9-b9fd-e10297897697","executionInfo":{"status":"ok","timestamp":1639536958716,"user_tz":300,"elapsed":162,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'text'],\n","        num_rows: 120000\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'text'],\n","        num_rows: 7600\n","    })\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"S2-2CbfPGYvi"},"source":["## Training\n","\n","We use a pre-trained RoBERTa model from HuggingFace. We use `RobertaModelWithHeads`, a class unique to `adapter-transformers`, which allows us to add and configure prediction heads in a flexibler way."]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"xBBeXc4FBK3m","executionInfo":{"status":"ok","timestamp":1639536963711,"user_tz":300,"elapsed":137,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tt9ZhlHxBK-l","executionInfo":{"status":"ok","timestamp":1639536963879,"user_tz":300,"elapsed":3,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"t9u8iaWCBLD9","executionInfo":{"status":"ok","timestamp":1639536964248,"user_tz":300,"elapsed":3,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["nLabels = len(np.unique(dataset['train']['labels']))\n","nLabels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGvFawhoA7p4","outputId":"ae3668a0-b8bf-4edd-bfb2-defa8bb7df05","executionInfo":{"status":"ok","timestamp":1639536965253,"user_tz":300,"elapsed":397,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[""],"metadata":{"id":"X5vA_aCwBJ65","executionInfo":{"status":"ok","timestamp":1639536967580,"user_tz":300,"elapsed":141,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":154,"referenced_widgets":["48a710c0529f474086c18c03d04c8bea","a4fe4e2a5f3f42f5a92caaef1c5801d1","4c1198131b654644846d81311b598f14","3b9878cf65ca40d5bd6a56488e37b505","bc88824f455b4aacb2cbffc51c6d39cd","8edb003ea2454d34aed083f99e1deb4f","b814154e478b4be7947e87c42749af3e","00f6ab358b7240c885109df9e905b0fe","8a8d9be2f7d047fa95ca162b56657447","457afdd7c9bd44a0807d9b93bbaea0ba","5334af46b5334678a72d01867dc121f7"]},"id":"Tp9uG-pT-qgv","outputId":"2845799a-6df2-40cf-ea25-e4ed728031aa","pycharm":{"is_executing":false},"executionInfo":{"status":"ok","timestamp":1639536989001,"user_tz":300,"elapsed":20512,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48a710c0529f474086c18c03d04c8bea","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import RobertaConfig, RobertaModelWithHeads\n","\n","config = RobertaConfig.from_pretrained(\n","    \"roberta-base\",\n","    num_labels=nLabels,\n",")\n","model = RobertaModelWithHeads.from_pretrained(\n","    \"roberta-base\",\n","    config=config,\n",")"]},{"cell_type":"markdown","metadata":{"id":"p0_oDQLJNKyO"},"source":["**Here comes the important part!**\n","\n","We add a new adapter to our model by calling `add_adapter()`. We pass a name (`\"rotten_tomatoes\"`) and [the type of adapter](https://docs.adapterhub.ml/adapters.html#adapter-types) (task adapter). Next, we add a binary classification head. It's convenient to give the prediction head the same name as the adapter. This allows us to activate both together in the next step. The `train_adapter()` method does two things:\n","\n","1. It freezes all weights of the pre-trained model so only the adapter weights are updated during training.\n","2. It activates the adapter and the prediction head such that both are used in every forward pass."]},{"cell_type":"code","source":["adapter_name = model.load_adapter(\"/content/drive/MyDrive/swp/AGnews Results/final_adapter\")"],"metadata":{"id":"EJk47slU6JCq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d725ee93-5a9a-4fb2-9c50-a0fbec008ad3","executionInfo":{"status":"ok","timestamp":1639537101844,"user_tz":300,"elapsed":133,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Overwriting existing adapter 'ag_news'.\n","Overwriting existing head 'ag_news'\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-28BRuSJfL4U","executionInfo":{"status":"ok","timestamp":1639537089268,"user_tz":300,"elapsed":155,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OB1xRVviMtP5","pycharm":{"is_executing":false}},"outputs":[],"source":["# # Add a new adapter\n","# model.add_adapter(\"sciie\")\n","# # Add a matching classification head\n","# model.add_classification_head(\n","#     \"sciie\",\n","#     num_labels=7,\n","#     # id2label={ 0: \"👎\", 1: \"👍\"}\n","#   )\n","\n","# # Activate the adapter\n","# model.train_adapter(\"sciie\")"]},{"cell_type":"code","source":["model.set_active_adapters(adapter_name)"],"metadata":{"id":"pzLznGYe_VK0","executionInfo":{"status":"ok","timestamp":1639537105213,"user_tz":300,"elapsed":129,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import torch\n","device='cuda'"],"metadata":{"id":"we45IvM6UMfs","executionInfo":{"status":"ok","timestamp":1639537107654,"user_tz":300,"elapsed":138,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# test_dataset[0]"],"metadata":{"id":"tHFB-7fUj-mD","executionInfo":{"status":"ok","timestamp":1639537109156,"user_tz":300,"elapsed":145,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["test_dataset = dataset['test']\n","bsz = 16\n","# bsz = test_dataset.num_rows\n","i = 0\n","batches = []\n","while i<test_dataset.num_rows:\n","  batches.append(test_dataset[i:i+bsz])\n","  i+=bsz\n","\n","  "],"metadata":{"id":"f-IbrN2iRXCT","executionInfo":{"status":"ok","timestamp":1639537145240,"user_tz":300,"elapsed":279,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Put model in evaluation mode\n","model.to(device)\n","model.eval()\n","\n","\n","# Tracking variables for storing ground truth and predictions \n","predictions , true_labels = [], []\n","\n","# Prediction Loop\n","for i,batch in enumerate(batches):\n","  print(i)\n","\n"," \n"," \n","  # Unpack the inputs from our dataloader and move to GPU/accelerator \n"," \n","  input_ids = batch['input_ids'].to(device)\n","  attention_mask = batch['attention_mask'].to(device)\n","  labels = batch['labels'].to(device)\n","\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(input_ids, attention_mask=attention_mask, \n","                         labels=labels)\n","\n","  logits = outputs[1]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"metadata":{"id":"2TpHo-g3-qND","executionInfo":{"status":"ok","timestamp":1639537426862,"user_tz":300,"elapsed":144927,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9243b1d5-4aa8-4c7e-eb8d-e9c15e2c0adc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n"]}]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt"],"metadata":{"id":"3W7_yk1jrBJS","executionInfo":{"status":"ok","timestamp":1639537435527,"user_tz":300,"elapsed":133,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# fig, a = plt.subplots(2,3)\n","# a[0,0].hist(logits[:,0])\n","# a[0,1].hist(logits[:,1])\n","# a[0,2].hist(logits[:,2])\n","# a[1,0].hist(logits[:,3])\n","# a[1,1].hist(logits[:,4])\n","# a[1,2].hist(logits[:,5])"],"metadata":{"id":"H26SePMjrmXR","executionInfo":{"status":"ok","timestamp":1639537436975,"user_tz":300,"elapsed":170,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["labels = np.array(true_labels).flatten()\n","prds = np.array(predictions).reshape(475*16,4).argmax(axis=1)"],"metadata":{"id":"ckdFdhJPkMye","executionInfo":{"status":"ok","timestamp":1639537630893,"user_tz":300,"elapsed":128,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"l3D7sy7PiBha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, classification_report"],"metadata":{"id":"8JLAliY8iSch","executionInfo":{"status":"ok","timestamp":1639537674071,"user_tz":300,"elapsed":363,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["f1_score(labels, prds, average='micro')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vatelT-DuJG3","executionInfo":{"status":"ok","timestamp":1639537674759,"user_tz":300,"elapsed":5,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}},"outputId":"2466af5e-d277-441c-ca9f-77a6fddb9333"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9500000000000001"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["print(classification_report(labels, prds, labels=list(range(nLabels)), digits=4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IX7mhHWtjY8S","executionInfo":{"status":"ok","timestamp":1639537676143,"user_tz":300,"elapsed":220,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}},"outputId":"16697f74-1024-42ad-909c-9005053bcb8f"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.9676    0.9574    0.9624      1900\n","           1     0.9889    0.9884    0.9887      1900\n","           2     0.9223    0.9189    0.9206      1900\n","           3     0.9217    0.9353    0.9284      1900\n","\n","    accuracy                         0.9500      7600\n","   macro avg     0.9501    0.9500    0.9500      7600\n","weighted avg     0.9501    0.9500    0.9500      7600\n","\n"]}]},{"cell_type":"code","source":["sum(dataset['train']['labels']==5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_LKfH7LlSbrl","outputId":"82690ecb-bb77-41ed-9ae3-ef2be987407f","executionInfo":{"status":"ok","timestamp":1639531776323,"user_tz":300,"elapsed":142,"user":{"displayName":"Xuanyu Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06212454760963057688"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(60)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# from datasets import list_metrics\n","# metrics_list = list_metrics()\n","# # len(metrics_list)\n","\n","# print(', '.join(metric for metric in metrics_list))"],"metadata":{"id":"BHks-8q1SauH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ev5t_8i8HzJB"},"source":["For training, we make use of the `Trainer` class built-in into `transformers`. We configure the training process using a `TrainingArguments` object and define a method that will calculate the evaluation accuracy in the end. We pass both, together with the training and validation split of our dataset, to the trainer instance.\n","\n","**Note the differences in hyperparameters compared to full finetuning.** Adapter training usually required a few more training epochs than full finetuning."]},{"cell_type":"code","source":[""],"metadata":{"id":"x0rFb4vI-pku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch, gc\n","\n","# gc.collect()\n","# torch.cuda.empty_cache()"],"metadata":{"id":"SXWWdipUUME4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FRft_5AAlQd","pycharm":{"is_executing":false}},"outputs":[],"source":["# import numpy as np\n","# from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n","\n","# training_args = TrainingArguments(\n","#     learning_rate=2*1e-5,\n","#     num_train_epochs=30,\n","#     per_device_train_batch_size=16,\n","#     per_device_eval_batch_size=16,\n","#     logging_steps=200,\n","#     output_dir=\"./training_output\",\n","#     overwrite_output_dir=True,\n","#     # The next line is important to ensure the dataset labels are properly passed to the model\n","#     remove_unused_columns=False,\n","# )\n","\n","# def compute_accuracy(p: EvalPrediction):\n","#   preds = np.argmax(p.predictions, axis=1)\n","#   return {\"acc\": (preds == p.label_ids).mean()}\n","\n","# trainer = AdapterTrainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=dataset[\"train\"],\n","#     eval_dataset=dataset[\"validation\"],\n","#     compute_metrics=compute_accuracy,\n","# )"]},{"cell_type":"markdown","metadata":{"id":"9iHhoYuLIdX3"},"source":["Start the training 🚀"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcZMwiJ_KDdP","pycharm":{"is_executing":false}},"outputs":[],"source":["# trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"KwyELxFKJJEz"},"source":["Looks good! Let's evaluate our adapter on the validation split of the dataset to see how well it learned:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PgHWEhsYeMv","pycharm":{"is_executing":false}},"outputs":[],"source":["# trainer.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"PLLWSmCjJbVX"},"source":["We can put our trained model into a `transformers` pipeline to be able to make new predictions conveniently:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Arw8GcsA8MeK","pycharm":{"is_executing":false}},"outputs":[],"source":["# from transformers import TextClassificationPipeline\n","\n","# classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n","\n","# classifier(\"This is awesome!\")"]},{"cell_type":"markdown","metadata":{"id":"cYykwoLaJvcJ"},"source":["At last, we can also extract the adapter from our model and separately save it for later reuse. Note the size difference compared to a full model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfK_AtSz4tlt","pycharm":{"is_executing":false}},"outputs":[],"source":["# model.save_adapter(\"./final_adapter\", \"sciie\")\n","\n","# !ls -lh final_adapter"]},{"cell_type":"markdown","metadata":{"id":"HE5osXd_eJ6t"},"source":["**Share your work!**\n","\n","The next step after training is to share our adapter with the world via _AdapterHub_. [Read our guide](https://docs.adapterhub.ml/contributing.html) on how to prepare the adapter module we just saved and contribute it to the Hub!\n","\n","➡️ Also continue with [the next Colab notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb) to learn how to use adapters from the Hub."]},{"cell_type":"code","source":["# from google.colab import files\n","# !zip -r /content/file.zip /content\n","# files.download(\"/content/file.zip\")"],"metadata":{"id":"TypkHGRaaql3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OMUIfuHphaql"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"AGNews-prd.ipynb","provenance":[{"file_id":"1ScLd0L1ts4hr4l-6tm0jrUFV6BxObVW0","timestamp":1639532566595}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a9e0c4cc805347be8ea8724d84b3153d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0b3f2e343c04d31934f9af5eff832ed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_02e58f4e62fe4d5fa7bb4e3b510549fc","IPY_MODEL_39fb423a6e9246df9536c8963e23d564","IPY_MODEL_dabc9f8c003d4066ab03b721d208cf52"]}},"d0b3f2e343c04d31934f9af5eff832ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02e58f4e62fe4d5fa7bb4e3b510549fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9650b69982ca41f9915d9546ccaa161b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ea18605f8b948c3ad7606d74e8d7260"}},"39fb423a6e9246df9536c8963e23d564":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_36cdb0b7705e4b7aa6d282fc1f730b18","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dca63249a5724b0a840b2c542707622b"}},"dabc9f8c003d4066ab03b721d208cf52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_edd5887dd82f484199b783256d21a936","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00, 47.21it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbcac12ef3ab4df88a9309a0f5ff8da8"}},"9650b69982ca41f9915d9546ccaa161b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ea18605f8b948c3ad7606d74e8d7260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"36cdb0b7705e4b7aa6d282fc1f730b18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dca63249a5724b0a840b2c542707622b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"edd5887dd82f484199b783256d21a936":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dbcac12ef3ab4df88a9309a0f5ff8da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8302c11f705d406392ec6d0911944dc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c486926c5c0c46c485a086d213f819bc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ff93d2b908f34b9ab8fb11aa72505883","IPY_MODEL_704f16ac62d94f459307a94ddec0428b","IPY_MODEL_9c813a83d84044bfa4d7aadf2a00546e"]}},"c486926c5c0c46c485a086d213f819bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff93d2b908f34b9ab8fb11aa72505883":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2d4f59f1d3514539b4f5bb6ec3a91a4d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bdec0bd6544340e9aa331608a5f9d2a1"}},"704f16ac62d94f459307a94ddec0428b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_61db1d37547d4c34bd0bc7314697b906","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":8,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_013ac438b1a541258bec00fc074073af"}},"9c813a83d84044bfa4d7aadf2a00546e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eea73dd7cb16451c955b46bbdd892db5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8/8 [00:04&lt;00:00,  2.07ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce0e3c61e8654d14aa727e8a6abfc988"}},"2d4f59f1d3514539b4f5bb6ec3a91a4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bdec0bd6544340e9aa331608a5f9d2a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61db1d37547d4c34bd0bc7314697b906":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"013ac438b1a541258bec00fc074073af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eea73dd7cb16451c955b46bbdd892db5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce0e3c61e8654d14aa727e8a6abfc988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48a710c0529f474086c18c03d04c8bea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a4fe4e2a5f3f42f5a92caaef1c5801d1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4c1198131b654644846d81311b598f14","IPY_MODEL_3b9878cf65ca40d5bd6a56488e37b505","IPY_MODEL_bc88824f455b4aacb2cbffc51c6d39cd"]}},"a4fe4e2a5f3f42f5a92caaef1c5801d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c1198131b654644846d81311b598f14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8edb003ea2454d34aed083f99e1deb4f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b814154e478b4be7947e87c42749af3e"}},"3b9878cf65ca40d5bd6a56488e37b505":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_00f6ab358b7240c885109df9e905b0fe","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a8d9be2f7d047fa95ca162b56657447"}},"bc88824f455b4aacb2cbffc51c6d39cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_457afdd7c9bd44a0807d9b93bbaea0ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 478M/478M [00:18&lt;00:00, 22.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5334af46b5334678a72d01867dc121f7"}},"8edb003ea2454d34aed083f99e1deb4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b814154e478b4be7947e87c42749af3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00f6ab358b7240c885109df9e905b0fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8a8d9be2f7d047fa95ca162b56657447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"457afdd7c9bd44a0807d9b93bbaea0ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5334af46b5334678a72d01867dc121f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":0}