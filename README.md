# Adapter vs Fine-tuning on NLP Transformer
<p>Replacing fine-tuning with Adapter using AdapterHub API .Our experiment will accross four domains and eight classification tasks as the table shown below:</p>
<img width="342" alt="Screen Shot 2021-12-06 at 5 48 29 PM" src="https://user-images.githubusercontent.com/32077985/144969779-48eef733-d9e1-4be5-bc9c-62e0f4bde332.png">

<p>Reference:</p>
<ol>Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks. Suchin Gururangan</ol>
<ol>AdapterHub: A Framework for Adapting Transformers. Jonas Pfeiffer</ol>
<ol>Parameter-Efficient Transfer Learning for NLP. Neil Houlsby</ol>
